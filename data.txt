Score analysis:
We started by creating a view for each of the data points which needed to be used in the z-score calculation from their table'd data which is joined onto the regions table that stores the sa2 region information. For public transport stops, we joined our Stops and Region table where the transport stop intersects the Region polygon, and then grouped by sa2 code and counted the number of different datapoints for each sa2 code. The same approach done for the primary and secondary catchements, and a similar approach was used for polling locations however st_contains was used instead of st_intersects. Retail stores and health services were found by joining our Business and Regions tables on the sa2 code, then grouping by sa2 code and counting the number of data points that have industry name as 'Retail Trade' for reatail stores view, and have industry name as 'Health Care and Social Assistance' for health services. We then created a population view for each sa2 region  that stored the sa2 code, the number of young people in that region, and the total number of people in the column, this is just to make it easier later on when calculating values for z-scores that only are supposed to use the number of young people. Once all the views were creating, we created a view that got had each datapoint for each region by joining all the previous views together. We then created a view that divided each of these datapoints by the number of 1000's of total people or 1000 of young people for the catchements data. To find all the z-scores for each datapoint, first a view was created which simply has the average and standard deviation for each of the columns in the per thousand view. Using this, we created the z_scores table which uses the z_score formula with the x values being the data points from the per thousand table, and using the avg and stddev from the view we just created. We also only inserted rows into this table where all z scores for the region were between 3 and -3, and outside these regions were considered as outliers which we didn't want to affect their final scores. We made the final column of this table a sum of all the z_scores with an exponentiation standardisation function applied to it to get all values between 0 and 1. We then added two more data points for each region, one was the amount of green space in the region, and the other was the number of enrolments in the region in 2022. We chose green space because places with parks, some bushland, and fields are often more desirable and give a more natural feeling to the region which most people enjoy. Enrolments was used as catchements desn't necessarily mean much, however lots of enrolments means there is a populus school in the region which is more desirable for families that want a big school(s) nearbly that their kids can get to easily. Both datasets had a geom attribute, and the same method as getting all the other data sets was used to find the number of enrolments and amount of green space per thousand people for each region, and the z scores of these, which were then appended onto the z_scores table, added to the total score value and had the exponentiation standardisation function applied to it. 

Correlation analysis:
The scores are not correlated much as the correlation was calculated to be around 0.5. This isn't suprising as the method for calculating the score of the region was not very accurate. For example most of the data points that were used don't take into account the quality of what is being summed. For example, a region may have a large number of retail stores and health services, however these aren't necessarily high quality. The other data points have similar problems, the number of transport stops isn't necessarily indicative of the quality of the neighbourhood, it may just be a popular connecting suburb on a transport line. Number of catchements also doesnt relate to which don't necessarily relate to the quality of a neighbourhood. Therefore it's not surprising that they were not very correlated. 